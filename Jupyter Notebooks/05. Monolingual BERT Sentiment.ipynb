{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 10358,
     "status": "ok",
     "timestamp": 1644131313624,
     "user": {
      "displayName": "Nigol Kalayjian",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16698750872284887439"
     },
     "user_tz": -120
    },
    "id": "JHd8FtytUpqw"
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30440,
     "status": "ok",
     "timestamp": 1644131344041,
     "user": {
      "displayName": "Nigol Kalayjian",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16698750872284887439"
     },
     "user_tz": -120
    },
    "id": "64rgfaYBUpq1",
    "outputId": "52469451-c5a4-40b4-9aba-0cf71887a52e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  135193344 \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 135,194,882\n",
      "Trainable params: 135,194,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#get the model\n",
    "model = TFBertForSequenceClassification.from_pretrained('./bert_model', from_pt = True)\n",
    "tokenizer = BertTokenizer.from_pretrained('./bert_model')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 1552,
     "status": "ok",
     "timestamp": 1644131345543,
     "user": {
      "displayName": "Nigol Kalayjian",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16698750872284887439"
     },
     "user_tz": -120
    },
    "id": "fm1cGJpEUpq2"
   },
   "outputs": [],
   "source": [
    "#read the data\n",
    "t = pd.read_csv('SentimentTrain_B.csv', encoding = 'utf-8')\n",
    "test = pd.read_csv('SentimentTest_B.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7968,
     "status": "ok",
     "timestamp": 1644131353502,
     "user": {
      "displayName": "Nigol Kalayjian",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16698750872284887439"
     },
     "user_tz": -120
    },
    "id": "IYTMV2idUpq3",
    "outputId": "fc8ba5a6-36de-44c7-e815-c77189c0c226"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "#create a polarity column and change the targets accordingly\n",
    "t['Polarity'] = np.nan\n",
    "for i in t.index:\n",
    "    if t['Column3'][i] == 'positive':\n",
    "        t['Polarity'][i] = 1\n",
    "    else:\n",
    "        t['Polarity'][i] = 0\n",
    "t = t.drop('Column3', axis = 1)\n",
    "\n",
    "test['Polarity'] = np.nan\n",
    "for i in test.index:\n",
    "    if test['Column3'][i] == 'positive':\n",
    "        test['Polarity'][i] = 1\n",
    "    else:\n",
    "        test['Polarity'][i] = 0\n",
    "test = test.drop('Column3', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1644131353503,
     "user": {
      "displayName": "Nigol Kalayjian",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16698750872284887439"
     },
     "user_tz": -120
    },
    "id": "L3H1w1xBUpq4"
   },
   "outputs": [],
   "source": [
    "#split data into train, validate\n",
    "train, dev = train_test_split(t, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1644131353503,
     "user": {
      "displayName": "Nigol Kalayjian",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16698750872284887439"
     },
     "user_tz": -120
    },
    "id": "XpUW_-79Upq5"
   },
   "outputs": [],
   "source": [
    "#define the functions\n",
    "def convert_data_to_examples(train, test, DATA_COLUMN, LABEL_COLUMN): \n",
    "    train_InputExamples = train.apply(lambda x: InputExample(guid=None, #globally unique ID for bookkeeping, unused in this case\n",
    "                                                          text_a = x[DATA_COLUMN], \n",
    "                                                          text_b = None,\n",
    "                                                          label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "    validation_InputExamples = test.apply(lambda x: InputExample(guid=None, #globally unique ID for bookkeeping, unused in this case\n",
    "                                                          text_a = x[DATA_COLUMN], \n",
    "                                                          text_b = None,\n",
    "                                                          label = x[LABEL_COLUMN]), axis = 1)\n",
    "  \n",
    "    return train_InputExamples, validation_InputExamples\n",
    "  \n",
    "def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n",
    "    features = [] #will hold InputFeatures to be converted later\n",
    "\n",
    "    for e in examples:\n",
    "        input_dict = tokenizer.encode_plus(\n",
    "            e.text_a,\n",
    "            add_special_tokens = True,\n",
    "            max_length = max_length, #truncates if len(s) > max_length\n",
    "            return_token_type_ids = True,\n",
    "            return_attention_mask = True,\n",
    "            pad_to_max_length = True, #pads to the right by default\n",
    "            truncation = True\n",
    "        )\n",
    "\n",
    "        input_ids, token_type_ids, attention_mask = (input_dict['input_ids'],\n",
    "            input_dict['token_type_ids'], input_dict['attention_mask'])\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                input_ids = input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids, label = e.label\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def gen():\n",
    "        for f in features:\n",
    "            yield (\n",
    "                {\n",
    "                    'input_ids': f.input_ids,\n",
    "                    'attention_mask': f.attention_mask,\n",
    "                    'token_type_ids': f.token_type_ids,\n",
    "                },\n",
    "                f.label,\n",
    "            )\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        ({'input_ids': tf.int32, 'attention_mask': tf.int32, 'token_type_ids': tf.int32}, tf.int64),\n",
    "        (\n",
    "            {\n",
    "                'input_ids': tf.TensorShape([None]),\n",
    "                'attention_mask': tf.TensorShape([None]),\n",
    "                'token_type_ids': tf.TensorShape([None]),\n",
    "            },\n",
    "            tf.TensorShape([]),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "DATA_COLUMN = 'Column4'\n",
    "LABEL_COLUMN = 'Polarity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1614563,
     "status": "ok",
     "timestamp": 1644132968033,
     "user": {
      "displayName": "Nigol Kalayjian",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16698750872284887439"
     },
     "user_tz": -120
    },
    "id": "Re8Vu11TUpq7",
    "outputId": "580f575d-ebaf-407b-a379-17e6d855c97f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2257: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1004/1004 - 1598s - loss: 0.2552 - accuracy: 0.8917 - val_loss: 0.3583 - val_accuracy: 0.8800 - 1598s/epoch - 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0232f1ff90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train the model on the dataset\n",
    "train_InputExamples, validation_InputExamples = convert_data_to_examples(train, dev, DATA_COLUMN, LABEL_COLUMN)\n",
    "\n",
    "train_data = convert_examples_to_tf_dataset(list(train_InputExamples), tokenizer)\n",
    "train_data = train_data.shuffle(100).batch(32).repeat(2)\n",
    "\n",
    "validation_data = convert_examples_to_tf_dataset(list(validation_InputExamples), tokenizer)\n",
    "validation_data = validation_data.batch(32)\n",
    "\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 3e-5),\n",
    "              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True), \n",
    "              metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n",
    "\n",
    "model.fit(train_data, epochs = 1, validation_data = validation_data, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9898,
     "status": "ok",
     "timestamp": 1644132977927,
     "user": {
      "displayName": "Nigol Kalayjian",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16698750872284887439"
     },
     "user_tz": -120
    },
    "id": "BOvGaqIyUpq8",
    "outputId": "0d315f35-7ce2-41b0-b9ec-83590581a7ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8251996450754214\n",
      "F-measure: 0.8928765633496465\n",
      "Recall: 0.9469434832756632\n",
      "Precision: 0.8446502057613169\n"
     ]
    }
   ],
   "source": [
    "#predict\n",
    "pred_sentences = test['Column4']\n",
    "tf_batch = tokenizer(list(pred_sentences), max_length = 128, padding = True, truncation = True, return_tensors = 'tf')\n",
    "tf_outputs = model(tf_batch)\n",
    "tf_predictions = tf.nn.softmax(tf_outputs[0], axis = -1)\n",
    "labels = [0, 1]\n",
    "label = tf.argmax(tf_predictions, axis = 1)\n",
    "label = label.numpy()\n",
    "predictions = pd.Series(label, index = test.index)\n",
    "print('Accuracy:', accuracy_score(test['Polarity'], predictions))\n",
    "print('F-measure:', f1_score(test['Polarity'], predictions))\n",
    "print('Recall:', recall_score(test['Polarity'], predictions))\n",
    "print('Precision:', precision_score(test['Polarity'], predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1644132977928,
     "user": {
      "displayName": "Nigol Kalayjian",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16698750872284887439"
     },
     "user_tz": -120
    },
    "id": "JXBgW3klUpq9",
    "outputId": "ed02d84a-8470-49fd-8c3e-d93ade3144b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[109, 151],\n",
       "       [ 46, 821]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "contingency_matrix(test['Polarity'], predictions)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "05. Monolingual BERT Sentiment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
