{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 10012,
     "status": "ok",
     "timestamp": 1644133155467,
     "user": {
      "displayName": "Nigol Kalayjian",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16698750872284887439"
     },
     "user_tz": -120
    },
    "id": "UcDMrNn4zBBY"
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19354,
     "status": "ok",
     "timestamp": 1644133174784,
     "user": {
      "displayName": "Nigol Kalayjian",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16698750872284887439"
     },
     "user_tz": -120
    },
    "id": "VMMTk7rYzBBm",
    "outputId": "ae1af25c-706c-4c53-d916-6137bb15c2ca"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  135193344 \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  3845      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 135,197,189\n",
      "Trainable params: 135,197,189\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#get the model\n",
    "model = TFBertForSequenceClassification.from_pretrained('./bert_model', from_pt = True, num_labels = 5)\n",
    "tokenizer = BertTokenizer.from_pretrained('./bert_model')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 409,
     "status": "ok",
     "timestamp": 1644133175182,
     "user": {
      "displayName": "Nigol Kalayjian",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16698750872284887439"
     },
     "user_tz": -120
    },
    "id": "pRWgFxb-zBBp"
   },
   "outputs": [],
   "source": [
    "#read the data\n",
    "t = pd.read_csv('EmotionTrain.csv', encoding = 'utf-8')\n",
    "test = pd.read_csv('EmotionTest.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1644133175186,
     "user": {
      "displayName": "Nigol Kalayjian",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16698750872284887439"
     },
     "user_tz": -120
    },
    "id": "Q2ezViBhzBBr"
   },
   "outputs": [],
   "source": [
    "#create a polarity column and change the targets accordingly\n",
    "def change_label(data):\n",
    "    data['Polarity'] = np.nan\n",
    "    for i in range(len(data)):\n",
    "        if data['Emotions'][i] == 'anger':\n",
    "            data['Polarity'][i] = 0\n",
    "        elif data['Emotions'][i] == 'fear':\n",
    "            data['Polarity'][i] = 1\n",
    "        elif data['Emotions'][i] == 'joy':\n",
    "            data['Polarity'][i] = 2\n",
    "        elif data['Emotions'][i] == 'sadness':\n",
    "            data['Polarity'][i] = 3\n",
    "        else:\n",
    "            data['Polarity'][i] = 4\n",
    "    data = data.drop('Emotions', axis = 1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4008,
     "status": "ok",
     "timestamp": 1644133179181,
     "user": {
      "displayName": "Nigol Kalayjian",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16698750872284887439"
     },
     "user_tz": -120
    },
    "id": "8YcCmwLZzBBs",
    "outputId": "d0b898ea-e209-4da1-b6e3-6669b3f5ae1e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "t = change_label(t)\n",
    "test = change_label(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1644133179183,
     "user": {
      "displayName": "Nigol Kalayjian",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16698750872284887439"
     },
     "user_tz": -120
    },
    "id": "tTGTpllkzBBu"
   },
   "outputs": [],
   "source": [
    "#split data into train, validate\n",
    "train, dev = train_test_split(t, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1644133179185,
     "user": {
      "displayName": "Nigol Kalayjian",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16698750872284887439"
     },
     "user_tz": -120
    },
    "id": "9y4qbz88zBBv"
   },
   "outputs": [],
   "source": [
    "#define the functions\n",
    "def convert_data_to_examples(train, test, DATA_COLUMN, LABEL_COLUMN): \n",
    "    train_InputExamples = train.apply(lambda x: InputExample(guid=None, #globally unique ID for bookkeeping, unused in this case\n",
    "                                                          text_a = x[DATA_COLUMN], \n",
    "                                                          text_b = None,\n",
    "                                                          label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "    validation_InputExamples = test.apply(lambda x: InputExample(guid=None, #globally unique ID for bookkeeping, unused in this case\n",
    "                                                          text_a = x[DATA_COLUMN], \n",
    "                                                          text_b = None,\n",
    "                                                          label = x[LABEL_COLUMN]), axis = 1)\n",
    "  \n",
    "    return train_InputExamples, validation_InputExamples\n",
    "  \n",
    "def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n",
    "    features = [] #will hold InputFeatures to be converted later\n",
    "\n",
    "    for e in examples:\n",
    "        input_dict = tokenizer.encode_plus(\n",
    "            e.text_a,\n",
    "            add_special_tokens = True,\n",
    "            max_length = max_length, #truncates if len(s) > max_length\n",
    "            return_token_type_ids = True,\n",
    "            return_attention_mask = True,\n",
    "            pad_to_max_length = True, #pads to the right by default\n",
    "            truncation = True\n",
    "        )\n",
    "\n",
    "        input_ids, token_type_ids, attention_mask = (input_dict['input_ids'],\n",
    "            input_dict['token_type_ids'], input_dict['attention_mask'])\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                input_ids = input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids, label = e.label\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def gen():\n",
    "        for f in features:\n",
    "            yield (\n",
    "                {\n",
    "                    'input_ids': f.input_ids,\n",
    "                    'attention_mask': f.attention_mask,\n",
    "                    'token_type_ids': f.token_type_ids,\n",
    "                },\n",
    "                f.label,\n",
    "            )\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        ({'input_ids': tf.int32, 'attention_mask': tf.int32, 'token_type_ids': tf.int32}, tf.int64),\n",
    "        (\n",
    "            {\n",
    "                'input_ids': tf.TensorShape([None]),\n",
    "                'attention_mask': tf.TensorShape([None]),\n",
    "                'token_type_ids': tf.TensorShape([None]),\n",
    "            },\n",
    "            tf.TensorShape([]),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "DATA_COLUMN = 'Tweet'\n",
    "LABEL_COLUMN = 'Polarity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 534567,
     "status": "ok",
     "timestamp": 1644133713733,
     "user": {
      "displayName": "Nigol Kalayjian",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16698750872284887439"
     },
     "user_tz": -120
    },
    "id": "AHyZf4h4zBB2",
    "outputId": "fe8e8187-d566-477e-c4ad-5986cb8659af"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2257: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232/232 - 484s - loss: 0.7010 - accuracy: 0.7459 - val_loss: 0.7580 - val_accuracy: 0.7561 - 484s/epoch - 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f092d7762d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train the model on the dataset\n",
    "train_InputExamples, validation_InputExamples = convert_data_to_examples(train, dev, DATA_COLUMN, LABEL_COLUMN)\n",
    "\n",
    "train_data = convert_examples_to_tf_dataset(list(train_InputExamples), tokenizer)\n",
    "train_data = train_data.shuffle(100).batch(32).repeat(2)\n",
    "\n",
    "validation_data = convert_examples_to_tf_dataset(list(validation_InputExamples), tokenizer)\n",
    "validation_data = validation_data.batch(32)\n",
    "\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 3e-5), \n",
    "              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True), \n",
    "              metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n",
    "\n",
    "model.fit(train_data, epochs = 1, validation_data = validation_data, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5936,
     "status": "ok",
     "timestamp": 1644133719653,
     "user": {
      "displayName": "Nigol Kalayjian",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16698750872284887439"
     },
     "user_tz": -120
    },
    "id": "gjUUGGGDzBB4",
    "outputId": "0c6fdf36-7e1c-42dd-869c-0a91382e5397"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7743682310469314\n",
      "F-measure: 0.7722586209079405\n",
      "Recall: 0.7743682310469314\n",
      "Precision: 0.7713549843194754\n"
     ]
    }
   ],
   "source": [
    "#predict\n",
    "pred_sentences = test['Tweet']\n",
    "tf_batch = tokenizer(list(pred_sentences), max_length = 128, padding = True, truncation = True, return_tensors = 'tf')\n",
    "tf_outputs = model(tf_batch)\n",
    "tf_predictions = tf.nn.softmax(tf_outputs[0], axis = -1)\n",
    "labels = [0, 1, 2, 3, 4]\n",
    "label = tf.argmax(tf_predictions, axis = 1)\n",
    "label = label.numpy()\n",
    "predictions = pd.Series(label, index = test.index)\n",
    "print('Accuracy:', accuracy_score(test['Polarity'], predictions))\n",
    "print('F-measure:', f1_score(test['Polarity'], predictions, average = 'weighted'))\n",
    "print('Recall:', recall_score(test['Polarity'], predictions, average = 'weighted'))\n",
    "print('Precision:', precision_score(test['Polarity'], predictions, average = 'weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1644133719654,
     "user": {
      "displayName": "Nigol Kalayjian",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16698750872284887439"
     },
     "user_tz": -120
    },
    "id": "82P6RiQKzBB5",
    "outputId": "40a78379-d062-4c08-85fa-8c1b6a2380fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 91,  12,  21,   8,   1],\n",
       "       [  6,  28,   7,   2,   0],\n",
       "       [ 13,   6, 256,  19,   0],\n",
       "       [ 14,   3,   7,  54,   0],\n",
       "       [  1,   1,   2,   2,   0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "contingency_matrix(test['Polarity'], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1644133719656,
     "user": {
      "displayName": "Nigol Kalayjian",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16698750872284887439"
     },
     "user_tz": -120
    },
    "id": "bc-Um0StzBB9",
    "outputId": "a677ac7b-b95e-4330-ba6b-6ebfb6696ba0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-measure: [0.70542636 0.60215054 0.87223169 0.66257669 0.        ]\n",
      "Recall: [0.68421053 0.65116279 0.8707483  0.69230769 0.        ]\n",
      "Precision: [0.728      0.56       0.87372014 0.63529412 0.        ]\n"
     ]
    }
   ],
   "source": [
    "print('F-measure:', f1_score(test['Polarity'], predictions, average = None))\n",
    "print('Recall:', recall_score(test['Polarity'], predictions, average = None))\n",
    "print('Precision:', precision_score(test['Polarity'], predictions, average = None))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "05. Monolingual BERT Emotions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
